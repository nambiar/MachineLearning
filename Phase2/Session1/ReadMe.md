### Understanding of the code

1. Word Embeddings of 100 dimension is enough to predict with 50-80% 

2. Tokenizer is word to sequence generated to be fed to embedding layer

3. Overfitting evident both in case of high and low number of samples

4. Model not taking into account the semantic relations i.e same adjective same result

5. Test accuracy of 87% with more number of samples

6. Data sparsity of interrelation between words minimal so no simple Dense gives good results

   #### Training & Validation Results
   ![Results](https://github.com/nambiar/MachineLearning/blob/master/Phase2/Session1/MoreSamples.JPG)
